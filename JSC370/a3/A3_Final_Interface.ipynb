{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UofT Journal Usage Stats\n",
    "\n",
    "### Authors: Tao, Shirley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import calendar\n",
    "import numpy as np\n",
    "import operator\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "from IPython.display import display\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "The data consists of the JR1 reports provided from the UofT librarians detailing the packages they currently have for all the journals UofT is subscribed to, along side the download counts for all of those journals, and the list of UofT publications for a year compiled from the Web of Science website.  Each JR1 report is an individual excel file, and the Web of Science listings are one excel file per year, so the data needs to be loaded and cleaned from its original form. This section loads the data into two dataframes.\n",
    "\n",
    "### Getting the Data\n",
    "1. Download \"Web of Science data UofT affiliated pubs 2014-2018.zip\" and \"JSC370 Data KM.zip\" from Quercus.\n",
    "2. Unzip both into the same folder as this notebook.\n",
    "3. In folders JSC370 Data KM and Web of Science data UofT affiliated pubs 2014-2018, delete the \"_MACOSX\" if there is one.\n",
    "4. Collapse the JSC370 Data KM/JSC370 Data KM folder into just a JSC370 Data KM folder.\n",
    "\n",
    "### Loading the Data\n",
    "**First Time**: If this is your first time using the notebook and you don't have the cleaned csv files of the data, follow the instructions above for getting the data, and run the FIRST and SECOND cells to load the data into the notebook.\n",
    "\n",
    "**For Future Usage**: The data can be saved to csv files so that future and repeated usage of this notebook allows the data to be loaded much faster.\n",
    "- Run the THIRD cell after running the first two to save the cleaned dataframes to csv files.\n",
    "- If you have the cleaned csv files, skip the first two cells and run the FOURTH cell to load the data.\n",
    "\n",
    "The first cell loads the JR1 reports, and will print out the file name and reason if it decides to not read in any files.  Currently we are skipping the CAIRN reports due to the French encoding issues and the excel files being formatted differently as a result of it.  The second cell loads the Web of Science UofT publications information.  The loading bar will turn green when the cell is finished running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1: JR1 reports\n",
    "\n",
    "# takes around 4 minutes to run\n",
    "\n",
    "# get report names\n",
    "reportnames = glob.glob('JSC370 Data KM/*.*', recursive = True)\n",
    "\n",
    "# create loading bar\n",
    "loading1 = widgets.IntProgress(value=0, min=0, max=len(reportnames) + 3, step=1, description='Loading:', \n",
    "                               bar_style='info', orientation='horizontal')\n",
    "display(loading1)\n",
    "\n",
    "# read in all excel files\n",
    "report_data = pd.DataFrame()\n",
    "for name in reportnames:\n",
    "    if \"JR1\" not in name:\n",
    "        print(name + \" Not Loaded: Not JR1\")\n",
    "        loading1.value = loading1.value + 1\n",
    "        continue\n",
    "    \n",
    "    excel = True\n",
    "    try:\n",
    "        curr_report = pd.read_excel(name)\n",
    "    except:\n",
    "        # corrupted xls files can be read in as csv files\n",
    "        # we know that for this set of data, the column names are either on the 7th or 9th row\n",
    "        try:\n",
    "            curr_report = pd.read_csv(name, skiprows=7, sep=\"\\t\")\n",
    "        except:\n",
    "            curr_report = pd.read_csv(name, skiprows=9, sep=\"\\t\")\n",
    "        excel = False\n",
    "    \n",
    "    if excel:\n",
    "        # get index where data starts \n",
    "        # (this is different between some files so using a loop to get the starting point)\n",
    "        colnamesindex = -1\n",
    "        for i in range(len(curr_report)):\n",
    "            if curr_report.iloc[i, 0] == \"Journal\" or curr_report.iloc[i, 1] == \"Publisher\":\n",
    "                colnamesindex = i\n",
    "                break\n",
    "        if colnamesindex == -1:\n",
    "            print(name + \" Not Loaded: Not in English, Not Formatted Right\")\n",
    "            loading1.value = loading1.value + 1\n",
    "            continue\n",
    "\n",
    "        # set column names\n",
    "        curr_report.columns = curr_report.loc[colnamesindex,]\n",
    "\n",
    "        if \"+\" in curr_report.columns:\n",
    "            # for the files formated weirdly with a + and spreading title names across rows\n",
    "            # fix column names\n",
    "            colnames = list(curr_report.columns)\n",
    "            for i in range(len(colnames)):\n",
    "                if pd.notna(curr_report.iloc[colnamesindex + 1, i]):\n",
    "                    colnames[i] = colnames[i] + \" \" + str(curr_report.iloc[colnamesindex + 1, i])\n",
    "                if pd.notna(curr_report.iloc[colnamesindex + 2, i]):\n",
    "                    colnames[i] = colnames[i] + \" \" + str(curr_report.iloc[colnamesindex + 2, i])\n",
    "            curr_report.columns = colnames\n",
    "            curr_report = curr_report.drop(columns=[\"+\"])\n",
    "            curr_report = curr_report.drop(curr_report[curr_report[\"Journal\"] == \"+\"].index, axis=0)\n",
    "\n",
    "            # save only data part\n",
    "            curr_report = curr_report[colnamesindex + 3:]\n",
    "        else:\n",
    "            # save only data part\n",
    "            curr_report = curr_report[colnamesindex + 1:]\n",
    "\n",
    "    # insert year of report\n",
    "    jr1_i = name.index(\"JR1\")\n",
    "    year = name[jr1_i + 4 : jr1_i + 8]\n",
    "    curr_report.insert(0, \"Year\", int(year))\n",
    "\n",
    "    # reformat months to not include year or date for generality\n",
    "    datetimes = {}\n",
    "    for colname in curr_report.columns:\n",
    "        if pd.notna(colname):\n",
    "            if isinstance(colname, datetime.datetime):\n",
    "                # some encoded as datetime\n",
    "                calendar.month_name[colname.month][:3]\n",
    "                datetimes[colname] = calendar.month_name[colname.month][:3]\n",
    "            elif isinstance(colname, str):\n",
    "                # sometimes there's whitespace messing things up\n",
    "                strippedcolname = colname.strip()\n",
    "                if strippedcolname.endswith(year):\n",
    "                    # some encoded as MMM-YYYY\n",
    "                    datetimes[colname] = strippedcolname[:3]\n",
    "                else:\n",
    "                    datetimes[colname] = strippedcolname\n",
    "    curr_report = curr_report.rename(columns=datetimes)\n",
    "\n",
    "    # for some reports that don't label the Journal column\n",
    "    curr_report = curr_report.rename(columns={np.nan: \"Journal\"})\n",
    "    \n",
    "    # insert file package name\n",
    "    package_i = name.index(\"\\\\\")\n",
    "    filepub = name[package_i + 1 : jr1_i - 1]\n",
    "    curr_report.insert(0, \"FilePackage\", filepub)\n",
    "    \n",
    "    # drop totals row\n",
    "    curr_report = curr_report.drop(curr_report.index[0])\n",
    "    \n",
    "    # insert if SP or not\n",
    "    mainname = name.split(\".\")[0]\n",
    "    if mainname.endswith(\"SP\"):\n",
    "        curr_report.insert(0, \"SP\", \"Yes\")\n",
    "    else:\n",
    "        curr_report.insert(0, \"SP\", \"No\")\n",
    "    \n",
    "    # append this excel file's data to the total dataframe\n",
    "    try:\n",
    "        report_data = report_data.append(curr_report)\n",
    "        loading1.value = loading1.value + 1 \n",
    "    except ValueError:\n",
    "        print(name + \" Not Loaded: Not Formatted Properly, Can't Append Data\")\n",
    "        loading1.value = loading1.value + 1\n",
    "        continue\n",
    "        \n",
    "\n",
    "# fixing more formatting problems\n",
    "report_data[\"Reporting Period Total\"] = report_data[\"Reporting Period Total\"].fillna(report_data[\"Retrievals\"])\n",
    "report_data[\"Reporting Period HTML\"] = report_data[\"Reporting Period HTML\"].fillna(report_data[\"HTML\"])\n",
    "report_data[\"Reporting Period PDF\"] = report_data[\"Reporting Period PDF\"].fillna(report_data[\"PDF\"])\n",
    "report_data[\"Journal DOI\"] = report_data[\"Journal DOI\"].fillna(report_data[\"Journal Doi\"])\n",
    "report_data[\"Journal\"] = report_data[\"Journal\"].fillna(report_data[\"Title\"])\n",
    "report_data[\"Journal\"] = report_data[\"Journal\"].fillna(report_data[\"Unnamed: 0\"])\n",
    "\n",
    "report_data = report_data.drop(columns=[\"Retrievals\", \"HTML\", \"PDF\", \"Journal Doi\", \"Title\", \"Dec-2015\", \"Unnamed: 0\"])\n",
    "report_data.loc[report_data[\"Online ISSN\"] == \" \", \"Online ISSN\"] = np.nan\n",
    "loading1.value = loading1.value + 1\n",
    "\n",
    "# drop the rows that are accidentally still there\n",
    "report_data = report_data[report_data[\"Reporting Period Total\"].notna()]\n",
    "\n",
    "# strip excess whitespace\n",
    "report_data[\"Journal\"] = report_data[\"Journal\"].apply(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "report_data[\"FilePackage\"] = report_data[\"FilePackage\"].apply(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "loading1.value = loading1.value + 1\n",
    "\n",
    "# double check data is correct\n",
    "report_data[\"Reporting Period Total\"] = report_data[\"Reporting Period Total\"].apply(\n",
    "    lambda x: x if isinstance(x, int) else np.nan)\n",
    "report_data = report_data.dropna(subset=[\"Reporting Period Total\"])\n",
    "report_data = report_data.reset_index(drop=True)\n",
    "\n",
    "# done\n",
    "loading1.value = loading1.value + 1\n",
    "loading1.bar_style = 'success'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2: Web of Science Uoft authors dataframe\n",
    "\n",
    "# uoft report names\n",
    "uoftnames = glob.glob('Web of Science data UofT affiliated pubs 2014-2018/*.*', recursive = True)\n",
    "\n",
    "# create loading bar\n",
    "loading2 = widgets.IntProgress(value=0, min=0, max=8, step=1, description='Loading:', \n",
    "                               bar_style='info', orientation='horizontal')\n",
    "display(loading2)\n",
    "\n",
    "# takes around 40 seconds to load\n",
    "uoft_data = pd.DataFrame()\n",
    "year = 2014\n",
    "for name in uoftnames:\n",
    "    curr_uoft = pd.read_excel(name)\n",
    "    curr_uoft.insert(0, \"Year\", year)\n",
    "    uoft_data = uoft_data.append(curr_uoft)\n",
    "    year = year + 1\n",
    "    loading2.value = loading2.value + 1\n",
    "    \n",
    "uoft_data = uoft_data.drop_duplicates()\n",
    "uoft_data = uoft_data.reset_index(drop=True)\n",
    "\n",
    "# fix formatting problems\n",
    "uoft_data[\"Category: Heading 1\"] = uoft_data[\"Category: Heading 1\"].fillna(uoft_data[\"Category: Headings 1\"])\n",
    "uoft_data[\"PubType\"] = uoft_data[\"PubType\"].fillna(uoft_data[\"Pubtype\"])\n",
    "uoft_data = uoft_data.drop(columns=[\"Category: Headings 1\", \"Pubtype\"])\n",
    "loading2.value = loading2.value + 1\n",
    "\n",
    "# count the number of uoft authors per publication, since the dataframe only gives a count of \n",
    "# total authors and lists out the uoft authors\n",
    "def get_num_uoft_authors(row):\n",
    "    new_row = row.copy()\n",
    "    first_three = [\"(a1) First UofT affiliated author's position in the author list \",\n",
    "                   \" (a2) Second UofT affiliated author's position in the author list\",\n",
    "                   \"(a3) Third UofT affiliated author's position in the author list \"]\n",
    "    num = 0\n",
    "    if pd.notna(row[first_three[0]]):\n",
    "        new_row[\"NumUofTAuthors\"] = new_row[\"NumUofTAuthors\"] + 1\n",
    "    else:\n",
    "        return new_row\n",
    "        \n",
    "    if pd.notna(row[first_three[1]]):\n",
    "        new_row[\"NumUofTAuthors\"] = new_row[\"NumUofTAuthors\"] + 1\n",
    "    else:\n",
    "        return new_row\n",
    "        \n",
    "    if pd.notna(row[first_three[2]]):\n",
    "        new_row[\"NumUofTAuthors\"] = new_row[\"NumUofTAuthors\"] + 1\n",
    "    else:\n",
    "        return new_row\n",
    "        \n",
    "    for j in range(4, 61):\n",
    "        if pd.notna(row['a' + str(j)]):\n",
    "            new_row[\"NumUofTAuthors\"] = new_row[\"NumUofTAuthors\"] + 1\n",
    "        else:\n",
    "            return new_row\n",
    "            \n",
    "    return new_row\n",
    "\n",
    "# takes a while to run\n",
    "uoft_data[\"NumUofTAuthors\"] = 0\n",
    "uoft_data = uoft_data.apply(get_num_uoft_authors, axis=1)\n",
    "loading2.value = loading2.value + 1\n",
    "\n",
    "uoft_data = uoft_data.reset_index(drop=True)\n",
    "loading2.value = loading2.value + 1\n",
    "loading2.bar_style = 'success'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL\n",
    "# Run this cell to save the data to a csv file so future loading is much faster\n",
    "report_data.to_csv(\"JSC370 Data KM.csv\")\n",
    "uoft_data.to_csv(\"Web of Science data UofT affiliated pubs 2014-2018.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aca690f1711d447a917ef6e71df55beb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, bar_style='info', description='Loading:', max=17)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# OPTIONAL\n",
    "# If the data has already been read in and cleaned and put into a csv before,\n",
    "# Run this to read it in\n",
    "\n",
    "# create loading bar\n",
    "loading5 = widgets.IntProgress(value=0, min=0, max=17, step=1, description='Loading:', \n",
    "                               bar_style='info', orientation='horizontal')\n",
    "display(loading5)\n",
    "\n",
    "# get all of the journals\n",
    "df = pd.read_csv(\"JSC370 Data KM.csv\").drop(\"Unnamed: 0\", axis = 1)\n",
    "report_data = df\n",
    "loading5.value = loading5.value + 1\n",
    "\n",
    "# get the uoft publication information\n",
    "uoft_data = pd.read_csv(\"Web of Science data UofT affiliated pubs 2014-2018.csv\")\n",
    "loading5.value = loading5.value + 1\n",
    "\n",
    "# reading from csv makes it think these are strings or floats, so convert to int\n",
    "report_data[\"Reporting Period Total\"] = report_data[\"Reporting Period Total\"].astype('Int64')\n",
    "loading5.value = loading5.value + 1\n",
    "\n",
    "report_data[\"Reporting Period HTML\"] = report_data[\"Reporting Period HTML\"].astype('Int64')\n",
    "loading5.value = loading5.value + 1\n",
    "\n",
    "report_data[\"Reporting Period PDF\"] = report_data[\"Reporting Period PDF\"].astype('Int64')\n",
    "loading5.value = loading5.value + 1\n",
    "\n",
    "months = [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
    "for month in months:\n",
    "    report_data[month] = report_data[month].astype('Int64')\n",
    "    loading5.value = loading5.value + 1\n",
    "    \n",
    "loading5.bar_style = 'success'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data selection functions used throughout the notebook\n",
    "\n",
    "# selects a subset of the data with the aggregated download counts for a month-year time range\n",
    "# of downloads (only year time range if download count HTML or PDF)\n",
    "# creates SettingWithCopyWarning even though it's fine\n",
    "def get_selected_report_data(type_of_downloads, start_year, start_month, end_year, end_month):\n",
    "    selected_data = pd.DataFrame()\n",
    "    months = [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
    "    \n",
    "    years = []  # years we need to compile download counts for\n",
    "    for i in range(end_year - start_year + 1):\n",
    "        years.append(start_year + i)\n",
    "\n",
    "    d_type = \"\"  # download type we want to count\n",
    "    if type_of_downloads == \"All\":\n",
    "        d_type = \"Reporting Period Total\"\n",
    "    elif type_of_downloads == \"PDF\":\n",
    "        d_type = \"Reporting Period PDF\"\n",
    "    elif type_of_downloads == \"HTML\":\n",
    "        d_type = \"Reporting Period HTML\"\n",
    "    else:\n",
    "        # download type not valid\n",
    "        print(\"Download Type not valid\")\n",
    "\n",
    "    # monthly specifics only available if download type is total\n",
    "    if type_of_downloads == \"All\":\n",
    "        start_month_i = months.index(start_month)\n",
    "        end_month_i = months.index(end_month)\n",
    "        if start_year == end_year:\n",
    "            # only within the year range\n",
    "            selected_data = report_data[report_data[\"Year\"] == start_year]\n",
    "            selected_data[\"Downloads\"] = 0\n",
    "\n",
    "            for j in range(start_month_i, end_month_i + 1):\n",
    "                selected_data[\"Downloads\"] = selected_data.loc[:, \"Downloads\"] + selected_data[months[j]].fillna(0)\n",
    "        \n",
    "        elif start_year < end_year:\n",
    "            # start year months\n",
    "            selected_data = report_data[report_data[\"Year\"] == start_year]\n",
    "            selected_data[\"Downloads\"] = 0\n",
    "\n",
    "            # get counts within month range for start year\n",
    "            for j in range(start_month_i, len(months)):\n",
    "                selected_data[\"Downloads\"] = selected_data.loc[:, \"Downloads\"] + selected_data[months[j]].fillna(0)\n",
    "\n",
    "            # inbetween years entire year counts\n",
    "            inbetween_years = years[1 : len(years) - 1]\n",
    "            inbetween_data = report_data[report_data[\"Year\"].isin(inbetween_years)]\n",
    "            inbetween_data[\"Downloads\"] = inbetween_data.loc[:, d_type]\n",
    "\n",
    "            # loop for end year months\n",
    "            end_data = report_data[report_data[\"Year\"] == end_year]\n",
    "            end_data[\"Downloads\"] = 0\n",
    "\n",
    "            for j in range(0, end_month_i + 1):\n",
    "                end_data[\"Downloads\"] = end_data.loc[:, \"Downloads\"] + end_data[months[j]].fillna(0)\n",
    "\n",
    "            selected_data = selected_data.append(inbetween_data)\n",
    "            selected_data = selected_data.append(end_data)\n",
    "\n",
    "        else:\n",
    "            # start year > end year, no data\n",
    "            selected_data = pd.DataFrame()\n",
    "    else:\n",
    "        # do only year selections\n",
    "        selected_data = report_data[report_data[\"Year\"].isin(years)]\n",
    "        selected_data[\"Downloads\"] = report_data[d_type]\n",
    "    \n",
    "    return selected_data\n",
    "\n",
    "\n",
    "# merges the selected subset of report data with the uoft publications dataframe\n",
    "# doesn't directly merge the two dataframes in case they're both huge to avoid memory errors\n",
    "def merge_selected_reports_and_uoft(selected_data):\n",
    "    # ISSNs of only selected data\n",
    "    available_links = selected_data[[\"Print ISSN\", \"Online ISSN\"]]\n",
    "    available_links = available_links.drop_duplicates()\n",
    "    available_links = available_links[(available_links[\"Print ISSN\"].notna()) | \n",
    "                                      (available_links[\"Online ISSN\"].notna())]\n",
    "    available_links = available_links.reset_index(drop=True)\n",
    "\n",
    "    # get uoft publications that match the selected ISSNs\n",
    "    matched_uoft = pd.merge(uoft_data, available_links, how=\"inner\", \n",
    "                            left_on=[\"ISSN\", \"eISSN\"], right_on=[\"Print ISSN\", \"Online ISSN\"])\n",
    "\n",
    "    # save their ISSNs\n",
    "    matched_uoft = matched_uoft.drop(columns=[\"ISSN\", \"eISSN\"])\n",
    "    available_links = matched_uoft[[\"Print ISSN\", \"Online ISSN\"]]\n",
    "    available_links = available_links.drop_duplicates()\n",
    "    available_links = available_links[(available_links[\"Print ISSN\"].notna()) | \n",
    "                                      (available_links[\"Online ISSN\"].notna())]\n",
    "    available_links = available_links.reset_index(drop=True)\n",
    "\n",
    "    # get download counts of those uoft publications\n",
    "    matched_selected = pd.merge(selected_data, available_links, how=\"inner\", \n",
    "                            on=[\"Print ISSN\", \"Online ISSN\"])\n",
    "\n",
    "    # takes a while and can cause memory problems if both dataframes really big\n",
    "    matched_data = pd.merge(matched_selected, matched_uoft, how=\"inner\", \n",
    "                            on=[\"Print ISSN\", \"Online ISSN\", \"Year\"]) #, right_on=[\"ISSN\", \"eISSN\"])\n",
    "    \n",
    "    matched_data = matched_data.reset_index(drop=True)\n",
    "    return matched_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Most Downloaded Journals\n",
    "\n",
    "This section should help answer which journals are downloaded the most.  It displays the most downloaded journals in:\n",
    "1. table form\n",
    "2. bar graph form with breakdown of platforms they're downloaded from\n",
    "3. bar graph form with breakdown of packages they're present in\n",
    "\n",
    "## How to Use the Widgets\n",
    "1. The Data Selection allows the user to choose a subset of the data that they want information for.  The user can choose to specify a month-year time range where they want the most downloaded journals.  They can also specify if they want all types of downloads or only PDF or HTML downloads, if they want to view Scholars Portal downloads or not, and if they only want journals from a specific Platform or Package.  Once the user is finished selecting the subset of data they want, click the Select Data button to load the graphs.  Make sure to press Select Data at the start to load data for display.\n",
    "2. The Display Selection allows the user to choose how many journals to show, and if it should show all journals or only journals with UofT publications.  Currently it can show the top 5, 10, or 15 journals by download count.  Changing a selection here will automatically update the graph.\n",
    "3. Hover the mouse over a bar in the bar graph to get more information about that journal in that group, its Publisher, and the number of downloads for that journal in that group.\n",
    "\n",
    "\n",
    "### Notes:\n",
    "- It can take a while to load the data for the graphs, especially if the selected time range is large.  The loading bar will turn green once all of the graphs have finished updating.\n",
    "- If the download type is All, the download counts are over the specified month-year range.  If the download type is HTML or PDF, the download counts are only over the year range specified.  This is due to the month breakdown of downloads only being available for total downloads. \n",
    "- If the produced table and graphs are empty, then the selected data subset has no information for your selected time range and specified choices.  Try increasing the time range or choosing less restrictive selections for the package or platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global dataframes used\n",
    "selected_data = None #get_selected_report_data(\"All\", 2014, \"Jan\", 2018, \"Dec\")\n",
    "uoft_reports = None #merge_selected_reports_and_uoft(selected_data)\n",
    "uoft_downloads = None #uoft_reports.drop_duplicates(subset=[\"Print ISSN\", \"Online ISSN\", \"Journal\", \"Year\", \"Platform\"])\n",
    "\n",
    "# current display options\n",
    "curr_data1 = selected_data\n",
    "top = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data selection widgets\n",
    "\n",
    "# date range slider\n",
    "dates = []\n",
    "for year in range(0, 5):\n",
    "    for month in range(1, 13):\n",
    "        dates.append(datetime.date(2014 + year, month, 1))\n",
    "options = [(i.strftime(' %b-%Y '), i) for i in dates]\n",
    "timerange1 = widgets.SelectionRangeSlider(options=options, index=(0,len(dates) - 1), \n",
    "                                          description=\"Time Range\", disabled=False,\n",
    "                                          layout={'width': '500px'})\n",
    "                   \n",
    "# download type dropdown\n",
    "download_dd1 = widgets.Dropdown(options = [\"All\", \"HTML\", \"PDF\"], \n",
    "                                description=\"Downloads:\")\n",
    "\n",
    "# Scholars Portal or not dropdown\n",
    "sp_dd1 = widgets.Dropdown(options = [\"All\", \"Only SP\", \"No SP\"], description=\"Choose SP:\")\n",
    "\n",
    "# platforms dropdown\n",
    "platforms_list = list(report_data[\"Platform\"].value_counts().index)\n",
    "platforms_list.sort()\n",
    "platforms_list = [\"All\"] + platforms_list\n",
    "platforms_dd1 = widgets.Dropdown(options = platforms_list, description=\"Platforms:\")\n",
    "\n",
    "# packages dropdown\n",
    "packages_list = list(report_data[\"FilePackage\"].value_counts().index)\n",
    "packages_list.sort()\n",
    "packages_list = [\"All\"] + packages_list\n",
    "packages_dd1 = widgets.Dropdown(options = packages_list, description=\"Packages:\")\n",
    "\n",
    "# select data button\n",
    "data_btn1 = widgets.Button(description='Select Data', disabled=False, button_style='')\n",
    "\n",
    "# put them all together in a box for display\n",
    "data_widgets1 = widgets.VBox([timerange1, download_dd1, sp_dd1, platforms_dd1, \n",
    "                              packages_dd1, data_btn1])\n",
    "\n",
    "##### \n",
    "\n",
    "# display selection widgets\n",
    "\n",
    "# dropdown menu of top\n",
    "top_dd = widgets.Dropdown(options = [5, 10, 15], description='Top:')\n",
    "\n",
    "# dropdown menu of if uoft or not\n",
    "uoft_dd = widgets.Dropdown(options = [\"All Reports\", \"Only UofT Reports\"], description=\"Type:\")\n",
    "input_widgets1 = widgets.HBox([top_dd, uoft_dd])\n",
    "\n",
    "#####\n",
    "\n",
    "# loading bar\n",
    "loading3 = widgets.IntProgress(value=0, min=0, max=10, step=1, description='Loading:', \n",
    "                               bar_style='info', orientation='horizontal')\n",
    "\n",
    "#####\n",
    "\n",
    "# output widgets\n",
    "counts_table = widgets.Output()\n",
    "platform_graph = go.FigureWidget() \n",
    "package_graph = go.FigureWidget() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions\n",
    "\n",
    "# get top_num journals by Downloads from data\n",
    "def get_top_journals(top_num, data):\n",
    "    # aggregate ratings across and different versions\n",
    "    aggregated = data.groupby([\"Journal\"])[\"Downloads\"].sum()\n",
    "    aggregated = pd.DataFrame(aggregated)\n",
    "    results = aggregated.sort_values(by=\"Downloads\", ascending=False)[0:top_num]\n",
    "    \n",
    "    # get order \n",
    "    order = results.reset_index()\n",
    "    order = order.drop(columns=[\"Downloads\"])\n",
    "    order = list(order[\"Journal\"])\n",
    "\n",
    "    # get publishers\n",
    "    top_journals = results.index\n",
    "    top_journals = data[data[\"Journal\"].isin(top_journals)]\n",
    "    publishers = top_journals.groupby([\"Journal\", \"Publisher\"])[\"Downloads\"].sum()\n",
    "    publishers = pd.DataFrame(publishers)\n",
    "    publishers = publishers.reset_index()\n",
    "    # each journal only has one publisher\n",
    "    # any extra publishers is just the same publisher but spelt slightly differently\n",
    "    publishers = publishers.drop_duplicates(subset=[\"Journal\"]) \n",
    "    publishers = publishers.drop(columns=[\"Downloads\"])\n",
    "    \n",
    "    # get platform counts\n",
    "    platforms = top_journals.groupby([\"Journal\", \"Platform\"])[\"Downloads\"].sum()\n",
    "    platforms = pd.DataFrame(platforms)\n",
    "    platforms = platforms.reset_index()\n",
    "    platforms = platforms.merge(publishers)\n",
    "\n",
    "    # get package counts\n",
    "    packages = top_journals.groupby([\"Journal\", \"FilePackage\"])[\"Downloads\"].sum()\n",
    "    packages = pd.DataFrame(packages)\n",
    "    packages = packages.reset_index()\n",
    "    packages = packages.merge(publishers)\n",
    "    \n",
    "    return results, order, platforms, packages\n",
    "\n",
    "\n",
    "# draw bar chart of downloads with breakdown of extra\n",
    "def barchart_of_downloads(data, extra, order, totals):\n",
    "    # draw a bar chart\n",
    "    fig = px.bar(data, x='Journal', y='Downloads', color=extra, \n",
    "                 hover_data=['Downloads', 'Publisher', extra],\n",
    "                 category_orders={\"Journal\": order}, height=600, width=900) \n",
    "\n",
    "    # draw number of downloads at the top of each bar\n",
    "    y1 = list(totals[\"Downloads\"])\n",
    "    xcoord = list(totals.index) \n",
    "    annotations = [dict(x=xi, y=yi, text=str(yi), xanchor='center', \n",
    "                        yanchor='bottom', showarrow=False) for xi, yi in zip(xcoord, y1)]\n",
    "\n",
    "    # add labels to the plot\n",
    "    title_part = \"Packages\"\n",
    "    if extra == \"Platform\":\n",
    "        title_part = \"Platforms\"\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title={'text': \"Top \" + str(top) + \" Journals and Their \" + title_part,\n",
    "               'y':0.95,\n",
    "               'x':0.5,\n",
    "               'xanchor': 'center',\n",
    "               'yanchor': 'top'},\n",
    "        annotations=annotations)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "# update the figures\n",
    "def update_journal_figs(results, order, platforms, packages):\n",
    "    # update counts table\n",
    "    counts_table.clear_output() \n",
    "    with counts_table:\n",
    "        display(results)\n",
    "    loading3.value = loading3.value + 1\n",
    "        \n",
    "    # update bar chart with platform breakdown\n",
    "    new_fig = barchart_of_downloads(platforms, \"Platform\", order, results)\n",
    "    platform_graph.data = []\n",
    "    platform_graph.add_traces(new_fig.data)\n",
    "    platform_graph.layout = new_fig.layout\n",
    "    loading3.value = loading3.value + 1\n",
    "    \n",
    "    # update bar chart with package breakdown\n",
    "    new_fig = barchart_of_downloads(packages, \"FilePackage\", order, results)\n",
    "    package_graph.data = []\n",
    "    package_graph.add_traces(new_fig.data)\n",
    "    package_graph.layout = new_fig.layout\n",
    "    loading3.value = loading3.value + 1\n",
    "\n",
    "\n",
    "# if top number to display is changed\n",
    "def top_update(change):  \n",
    "    global top\n",
    "    counts_table.clear_output() \n",
    "    \n",
    "    loading3.value = 0\n",
    "    loading3.bar_style = \"\"\n",
    "    loading3.value = 5\n",
    "    \n",
    "    # don't need to change current data\n",
    "    top = change.new\n",
    "    results, order, platforms, packages = get_top_journals(top, curr_data1)\n",
    "    loading3.value = loading3.value + 1\n",
    "    \n",
    "    update_journal_figs(results, order, platforms, packages)\n",
    "    loading3.value = loading3.value + 1\n",
    "    loading3.bar_style = \"success\"\n",
    "        \n",
    "\n",
    "# if All or Only Uoft reports is changed\n",
    "def uoft_update(change):  \n",
    "    global curr_data1\n",
    "    counts_table.clear_output() \n",
    "    \n",
    "    loading3.value = 0\n",
    "    loading3.bar_style = \"\"\n",
    "    loading3.value = 5\n",
    "    \n",
    "    if change.new == \"All Reports\":\n",
    "        curr_data1 = selected_data\n",
    "    elif change.new == \"Only UofT Reports\":\n",
    "        curr_data1 = uoft_downloads\n",
    "    \n",
    "    results, order, platforms, packages = get_top_journals(top, curr_data1)\n",
    "    loading3.value = loading3.value + 1\n",
    "    \n",
    "    update_journal_figs(results, order, platforms, packages)\n",
    "    loading3.value = loading3.value + 1\n",
    "    loading3.bar_style = \"success\"\n",
    "\n",
    "    \n",
    "# if data selection in Part 1 is changed\n",
    "def select_data_update1(change):\n",
    "    global selected_data\n",
    "    global uoft_reports\n",
    "    global uoft_downloads\n",
    "    global curr_data1\n",
    "    \n",
    "    loading3.value = 0\n",
    "    loading3.bar_style = \"\"\n",
    "    \n",
    "    # get stuff from selection widgets\n",
    "    start, end = timerange1.get_state()[\"index\"] \n",
    "    start_year = dates[start].year\n",
    "    start_month = dates[start].strftime('%b')\n",
    "    end_year = dates[end].year\n",
    "    end_month = dates[end].strftime('%b')\n",
    "    type_of_downloads = download_dd1.value\n",
    "    sp_selection = sp_dd1.value\n",
    "    platform_selection = platforms_dd1.value\n",
    "    package_selection = packages_dd1.value\n",
    "    \n",
    "    # get selected data\n",
    "    selected_data = get_selected_report_data(type_of_downloads, start_year, start_month, end_year, end_month)\n",
    "    loading3.value = loading3.value + 1\n",
    "    \n",
    "    # filter further\n",
    "    if sp_selection == \"Only SP\":\n",
    "        selected_data = selected_data[selected_data[\"SP\"] == \"Yes\"]\n",
    "    if sp_selection == \"No SP\":\n",
    "        selected_data = selected_data[selected_data[\"SP\"] == \"No\"]\n",
    "    if platform_selection != \"All\":\n",
    "        selected_data = selected_data[selected_data[\"Platform\"] == platform_selection]\n",
    "    if package_selection != \"All\":\n",
    "        selected_data = selected_data[selected_data[\"FilePackage\"] == package_selection]\n",
    "    loading3.value = loading3.value + 1\n",
    "    \n",
    "    # get uoft publications joined with selected data\n",
    "    uoft_reports = merge_selected_reports_and_uoft(selected_data)\n",
    "    loading3.value = loading3.value + 1\n",
    "    \n",
    "    # save only 1 of each journal that has uoft publications in it\n",
    "    uoft_downloads = uoft_reports.drop_duplicates(\n",
    "        subset=[\"Print ISSN\", \"Online ISSN\", \"Journal\", \"Year\", \"Platform\"])\n",
    "    loading3.value = loading3.value + 1\n",
    "    \n",
    "    # save data from display selection\n",
    "    if uoft_dd.value == \"All Reports\":\n",
    "        curr_data1 = selected_data\n",
    "    else:\n",
    "        curr_data1 = uoft_downloads\n",
    "    loading3.value = loading3.value + 1\n",
    "    \n",
    "    # get data for graphing\n",
    "    results, order, platforms, packages = get_top_journals(top_dd.value, curr_data1)\n",
    "    loading3.value = loading3.value + 1\n",
    "    \n",
    "    # draw figures\n",
    "    update_journal_figs(results, order, platforms, packages)\n",
    "    loading3.value = loading3.value + 1\n",
    "    loading3.bar_style = \"success\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update when needed\n",
    "top_dd.observe(top_update, names = 'value')\n",
    "uoft_dd.observe(uoft_update, names = 'value')\n",
    "data_btn1.on_click(select_data_update1)\n",
    "\n",
    "# selections in folding accordian tabs\n",
    "selections1 = widgets.Accordion(children=[data_widgets1, input_widgets1])\n",
    "selections1.set_title(0, 'Data Selection')\n",
    "selections1.set_title(1, 'Display Selection')\n",
    "\n",
    "# output graph tabs\n",
    "tab1 = widgets.Tab([counts_table, platform_graph, package_graph])\n",
    "tab1.set_title(0, \"Counts\")\n",
    "tab1.set_title(1, 'Graph With Platform')\n",
    "tab1.set_title(2, 'Graph With Package')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eea2a43f0fb4a8894e7dbe148917e4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(VBox(children=(SelectionRangeSlider(description='Time Range', index=(0, 59), layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d680451c75646cca47b6bf738f697f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, bar_style='info', description='Loading:', max=10)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de5a6e5da0ae483db847838801d62194",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(Output(), FigureWidget({\n",
       "    'data': [], 'layout': {'template': '...'}\n",
       "}), FigureWidget({\n",
       "    'd…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(selections1)\n",
    "display(loading3)\n",
    "display(tab1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: UofT Journals: Downloads, Authors, Categories\n",
    "\n",
    "This section should help give an idea of what the journals that currently have uoft publications in them are like.\n",
    "- Stats: Some basic stats about the number and proportion of uoft publications.\n",
    "- Distributions: The distribution of total downloads, number of uoft authors per publication, proportion of uoft authors to all authors of the publication, and number of pages per publication.\n",
    "- Journal Types: A breakdown of publication types and document types.\n",
    "- Categories: A breakdown of heading, subheading, and subject categories.\n",
    "\n",
    "## How to Use the Widgets\n",
    "1. The Data Selection works the same way as it does in Part 1.  It allows the user to choose a subset of the data that they want information for.  Make sure to press Select Data at the start to load data for display.\n",
    "\n",
    "2. The Display Selection allows the user to also choose a subset of data based on heading category and subheading category.\n",
    "\n",
    "3. In the pie charts, clicking an item in the legend will omit it from the pie chart, to help gain an idea of what the distribution is like without that element, and clicking it again will bring it back.\n",
    "\n",
    "\n",
    "### Notes:\n",
    "- A journal can have multiple heading and subheading categories.  Even if a heading or subheading category is specified, other categories can show up in the pie chart, because a publication has both subheadings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframes\n",
    "selected_data2 = None \n",
    "uoft_reports2 = None \n",
    "uoft_downloads2 = None \n",
    "\n",
    "# selections\n",
    "head_selection = \"All Headings\"\n",
    "subhead_selection = \"All Subheadings\"\n",
    "curr_data2 = uoft_reports2\n",
    "curr_downloads = uoft_downloads2\n",
    "proportions = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions\n",
    "# select by topic\n",
    "def select_topics(head, subhead, uoft_selected_data):\n",
    "    if head != \"All Headings\":\n",
    "        has_head = []\n",
    "        # actually gotta check each individual one to see if we include it or not\n",
    "        for i, r in uoft_selected_data.iterrows():\n",
    "            if pd.notna(r[\"Category: Heading 1\"]):\n",
    "                if head in r[\"Category: Heading 1\"]:\n",
    "                    has_head.append(i)\n",
    "        curr_data2 = uoft_selected_data.loc[has_head,:]\n",
    "    else:\n",
    "        curr_data2 = uoft_selected_data\n",
    "        \n",
    "    if subhead != \"All Subheadings\":\n",
    "        has_subhead = []\n",
    "        # actually gotta check each individual one to see if we include it or not\n",
    "        for i, r in curr_data2.iterrows():\n",
    "            if pd.notna(r[\"Category: Subheadings\"]):\n",
    "                if subhead in r[\"Category: Subheadings\"]:\n",
    "                    has_subhead.append(i)\n",
    "        curr_data2 = curr_data2.loc[has_subhead,:]\n",
    "        \n",
    "    # unique downloads cause there are sometimes multiple publications per journal\n",
    "    unique_downloads = curr_data2.drop_duplicates(\n",
    "        subset=[\"Print ISSN\", \"Online ISSN\", \"Journal\", \"Year\", \"Platform\"])\n",
    "    return curr_data2, unique_downloads\n",
    "\n",
    "\n",
    "# get stats about the number and proportion of uoft publications\n",
    "def get_stats(all_data, download_data):\n",
    "    stats = {\"Number of UofT publications in journals subscribed to:\": str(len(all_data)),\n",
    "             \"Number of UofT authors of publications in journals subscribed to:\": str(sum(all_data[\"NumUofTAuthors\"])),\n",
    "             \"Number of journals with UofT publications subscribed to:\": str(len(download_data)),\n",
    "             \"Percentage of journals with UofT publications subscribed to:\": \n",
    "              str(round((len(download_data) / len(selected_data2)) * 100, 3)) + \"%\"\n",
    "            }\n",
    "    stats = pd.DataFrame(stats.items(), columns=[\"Stat\", \"Value\"])\n",
    "    stats = stats.set_index(\"Stat\")\n",
    "    return stats\n",
    "\n",
    "\n",
    "# get counts of document types\n",
    "def get_doctype_counts(all_data):\n",
    "    doctype_counts = all_data[\"Document Type\"].value_counts()\n",
    "    indexes = list(doctype_counts.index)\n",
    "    curr_counts = list(doctype_counts.values)\n",
    "\n",
    "    i = 0\n",
    "    counts = {}\n",
    "    for doctype in indexes:\n",
    "        types = doctype.replace(',',';').split(\";\")\n",
    "        for onetype in types:\n",
    "            onetype = onetype.strip()\n",
    "            if onetype not in counts:\n",
    "                counts[onetype] = 0\n",
    "            counts[onetype] = counts[onetype] + curr_counts[i]\n",
    "        i = i + 1\n",
    "    return counts\n",
    "\n",
    "\n",
    "# get category counts\n",
    "def get_cat_counts(all_data, column):\n",
    "    cat_counts = all_data[column].value_counts()\n",
    "    indexes = list(cat_counts.index)\n",
    "    curr_counts = list(cat_counts.values)\n",
    "\n",
    "    i = 0\n",
    "    cat_counts = {}\n",
    "    for cat in indexes:\n",
    "        types = cat.replace(',',';').split(\";\")\n",
    "        for onetype in types:\n",
    "            onetype = onetype.strip()\n",
    "            if onetype not in cat_counts:\n",
    "                cat_counts[onetype] = 0\n",
    "            cat_counts[onetype] = cat_counts[onetype] + curr_counts[i]\n",
    "        i = i + 1\n",
    "        \n",
    "    return cat_counts\n",
    "\n",
    "\n",
    "# get subject breakdown counts\n",
    "def get_subject_counts(all_data):\n",
    "    cat3_counts = all_data[\"Category: Subjects\"].value_counts()\n",
    "    indexes = list(cat3_counts.index)\n",
    "    curr_counts = list(cat3_counts.values)\n",
    "\n",
    "    i = 0\n",
    "    cat3_counts = {}\n",
    "    for cat3 in indexes:\n",
    "        types = cat3.replace(',',';').split(\";\")\n",
    "        for onetype in types:\n",
    "            # also turn it all lowercase since sometimes the subject appears as all caps and \n",
    "            # sometimes only the first character is capitalized\n",
    "            onetype = onetype.strip().lower()\n",
    "            if onetype not in cat3_counts:\n",
    "                cat3_counts[onetype] = 0\n",
    "            cat3_counts[onetype] = cat3_counts[onetype] + curr_counts[i]\n",
    "        i = i + 1\n",
    "\n",
    "    all_cat3_counts = cat3_counts.copy()\n",
    "    max_subject = max(all_cat3_counts.items(), key=operator.itemgetter(1))[0]\n",
    "    bound = cat3_counts[max_subject] / 4\n",
    "\n",
    "    small = {}\n",
    "    total = 0\n",
    "    for cat3 in cat3_counts:\n",
    "        if cat3_counts[cat3] < bound:\n",
    "            small[cat3] = cat3_counts[cat3]\n",
    "            total = total + cat3_counts[cat3]\n",
    "\n",
    "    too_small = list(small.keys())\n",
    "    for key in too_small:\n",
    "        del cat3_counts[key]\n",
    "    cat3_counts[\"Other\"] = total\n",
    "\n",
    "    all_cat3_data = pd.DataFrame(all_cat3_counts.items(), columns=['Subject', 'Number of Publications'])\n",
    "    cat3_data = pd.DataFrame(cat3_counts.items(), columns=[\"Subject\", \"Num Publications\"])\n",
    "    return cat3_data, all_cat3_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data selection widgets\n",
    "\n",
    "# time range slider\n",
    "timerange2 = widgets.SelectionRangeSlider(options=options, index=(0,len(dates) - 1), \n",
    "                                          description=\"Time Range\", disabled=False,\n",
    "                                          layout={'width': '500px'})\n",
    "\n",
    "# download type selection dropdown\n",
    "download_dd2 = widgets.Dropdown(options = [\"All\", \"HTML\", \"PDF\"], \n",
    "                                description=\"Downloads:\")\n",
    "\n",
    "# Scholars Portal selection dropdown\n",
    "sp_dd2 = widgets.Dropdown(options = [\"All\", \"Only SP\", \"No SP\"], description=\"Choose SP:\")\n",
    "\n",
    "# platforms dropdown\n",
    "platforms_dd2 = widgets.Dropdown(options = platforms_list, description=\"Platforms:\")\n",
    "\n",
    "# packages dropdown\n",
    "packages_dd2 = widgets.Dropdown(options = packages_list, description=\"Packages:\")\n",
    "\n",
    "# Select Data button\n",
    "data_btn2 = widgets.Button(description='Select Data', disabled=False, button_style='')\n",
    "\n",
    "# put together in a box\n",
    "data_widgets2 = widgets.VBox([timerange2, download_dd2, sp_dd2, platforms_dd2, \n",
    "                              packages_dd2, data_btn2])\n",
    "\n",
    "##### \n",
    "\n",
    "# display widgets\n",
    "\n",
    "# headings dropdown\n",
    "head_dd = widgets.Dropdown(options = [\"All Headings\"] + \n",
    "                           list(get_cat_counts(uoft_data, \"Category: Heading 1\").keys()))\n",
    "\n",
    "# subheadings dropdown\n",
    "subhead_dd = widgets.Dropdown(options = [\"All Subheadings\"] + \n",
    "                              list(get_cat_counts(uoft_data, \"Category: Subheadings\").keys()))\n",
    "\n",
    "# put together in a box\n",
    "input_widgets2 = widgets.HBox([head_dd, subhead_dd])\n",
    "\n",
    "#####\n",
    "\n",
    "# loading bar\n",
    "loading4 = widgets.IntProgress(value=0, min=0, max=15, step=1, description='Loading:', \n",
    "                               bar_style='info', orientation='horizontal')\n",
    "\n",
    "#####\n",
    "\n",
    "# output widgets\n",
    "\n",
    "# stats\n",
    "uoft_stats = widgets.Output()\n",
    "\n",
    "# distributions\n",
    "download_dist = go.FigureWidget()\n",
    "num_authors_dist = go.FigureWidget()\n",
    "prop_authors_dist = go.FigureWidget()\n",
    "num_pages_dist = go.FigureWidget()\n",
    "\n",
    "# journal types\n",
    "pubtype_dist = go.FigureWidget()\n",
    "doctype_dist = go.FigureWidget()\n",
    "\n",
    "# categories\n",
    "cat1_graph = go.FigureWidget()\n",
    "cat2_graph = go.FigureWidget()\n",
    "cat3_graph = go.FigureWidget()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update functions\n",
    "\n",
    "# select data based on data selections\n",
    "def select_data_update2(change):\n",
    "    global selected_data2\n",
    "    global uoft_reports2\n",
    "    global uoft_downloads2\n",
    "    global curr_data2\n",
    "    global curr_downloads\n",
    "    global proportions\n",
    "    uoft_stats.clear_output() \n",
    "    \n",
    "    loading4.bar_style = \"\"\n",
    "    loading4.value = 0\n",
    "    \n",
    "    start, end = timerange2.get_state()[\"index\"] \n",
    "    start_year = dates[start].year\n",
    "    start_month = dates[start].strftime('%b')\n",
    "    end_year = dates[end].year\n",
    "    end_month = dates[end].strftime('%b')\n",
    "    type_of_downloads = download_dd2.value\n",
    "    sp_selection = sp_dd2.value\n",
    "    platform_selection = platforms_dd2.value\n",
    "    package_selection = packages_dd2.value\n",
    "    \n",
    "    selected_data2 = get_selected_report_data(type_of_downloads, start_year, start_month, end_year, end_month)\n",
    "    loading4.value = loading4.value + 1\n",
    "    \n",
    "    if sp_selection == \"Only SP\":\n",
    "        selected_data2 = selected_data2[selected_data2[\"SP\"] == \"Yes\"]\n",
    "    if sp_selection == \"No SP\":\n",
    "        selected_data2 = selected_data2[selected_data2[\"SP\"] == \"No\"]\n",
    "    if platform_selection != \"All\":\n",
    "        selected_data2 = selected_data2[selected_data2[\"Platform\"] == platform_selection]\n",
    "    if package_selection != \"All\":\n",
    "        selected_data2 = selected_data2[selected_data2[\"FilePackage\"] == package_selection]\n",
    "    loading4.value = loading4.value + 1\n",
    "    \n",
    "    uoft_reports2 = merge_selected_reports_and_uoft(selected_data2)\n",
    "    loading4.value = loading4.value + 1\n",
    "    \n",
    "    loading4.value = loading4.value + 1\n",
    "    \n",
    "    curr_data2, curr_downloads = select_topics(head_selection, subhead_selection, uoft_reports2)\n",
    "    proportions = pd.DataFrame(curr_data2[\"NumUofTAuthors\"] / \n",
    "                               curr_data2[\"Total number of authors\"]).rename(columns={0: \"Proportions\"})\n",
    "    update_uoft_figs(curr_data2, curr_downloads, proportions)\n",
    "    \n",
    "    loading4.value = loading4.value + 1\n",
    "    loading4.bar_style = \"success\"\n",
    "    \n",
    "\n",
    "# heading selection changed, update graphs\n",
    "def head_update(change):  \n",
    "    global head_selection\n",
    "    global curr_data2\n",
    "    global curr_downloads\n",
    "    global proportions\n",
    "    uoft_stats.clear_output() \n",
    "    \n",
    "    loading4.bar_style = \"\"\n",
    "    loading4.value = 5\n",
    "    \n",
    "    head_selection = change.new\n",
    "    curr_data2, curr_downloads = select_topics(head_selection, subhead_selection, uoft_reports2)\n",
    "    proportions = curr_data2[\"NumUofTAuthors\"] / curr_data2[\"Total number of authors\"]\n",
    "    proportions = pd.DataFrame(proportions).rename(columns={0: \"Proportions\"})\n",
    "    update_uoft_figs(curr_data2, curr_downloads, proportions)\n",
    "    \n",
    "    loading4.value = loading4.value + 1\n",
    "    loading4.bar_style = \"success\"\n",
    "    \n",
    "\n",
    "# subheading selection changed, update graphs\n",
    "def subhead_update(change):  \n",
    "    global subhead_selection\n",
    "    global curr_data2\n",
    "    global curr_downloads\n",
    "    global proportions\n",
    "    uoft_stats.clear_output() \n",
    "    \n",
    "    loading4.bar_style = \"\"\n",
    "    loading4.value = 5\n",
    "    \n",
    "    subhead_selection = change.new\n",
    "    curr_data2, curr_downloads = select_topics(head_selection, subhead_selection, uoft_reports2)\n",
    "    proportions = curr_data2[\"NumUofTAuthors\"] / curr_data2[\"Total number of authors\"]\n",
    "    proportions = pd.DataFrame(proportions).rename(columns={0: \"Proportions\"})\n",
    "    update_uoft_figs(curr_data2, curr_downloads, proportions)\n",
    "    \n",
    "    loading4.value = loading4.value + 1\n",
    "    loading4.bar_style = \"success\"\n",
    "    \n",
    "    \n",
    "# update the distributions and stats\n",
    "def update_uoft_figs(curr_data2, curr_downloads, proportions):\n",
    "    with uoft_stats:\n",
    "        display(get_stats(curr_data2, curr_downloads))\n",
    "    loading4.value = loading4.value + 1\n",
    "    \n",
    "    # update download distribution\n",
    "    new_fig = px.histogram(curr_downloads, x=\"Downloads\", marginal=\"box\", nbins=200, \n",
    "                           title=\"Downloads of Journals with UofT Publications\", height=400)\n",
    "    download_dist.data = []\n",
    "    download_dist.add_traces(new_fig.data)\n",
    "    download_dist.layout = new_fig.layout\n",
    "    loading4.value = loading4.value + 1\n",
    "    \n",
    "    # update proportion authors distribution\n",
    "    fig2 = px.histogram(proportions, x=\"Proportions\", nbins=20, \n",
    "                        title=\"Proportion of UofT Authors in Publications\", height=400,\n",
    "                        labels={\"Proportions\": \"Number of UofT Authors / Total Number of Authors\"})\n",
    "    prop_authors_dist.data = []\n",
    "    prop_authors_dist.add_traces(fig2.data)\n",
    "    prop_authors_dist.layout = fig2.layout\n",
    "    loading4.value = loading4.value + 1\n",
    "        \n",
    "    # update num authors distribution\n",
    "    fig1 = px.histogram(curr_data2, x=\"NumUofTAuthors\", marginal=\"box\", nbins=100, \n",
    "                        title=\"Number of UofT Authors in Publications\", height=400,\n",
    "                        labels={\"NumUofTAuthors\": \"Number of UofT Authors\"})\n",
    "    num_authors_dist.data = []\n",
    "    num_authors_dist.add_traces(fig1.data)\n",
    "    num_authors_dist.layout = fig1.layout\n",
    "    loading4.value = loading4.value + 1\n",
    "        \n",
    "    # update num pages\n",
    "    fig3 = px.histogram(curr_data2, x=\"Number of Pages\", marginal=\"box\", nbins=100, \n",
    "                        title=\"Number of Pages in UofT Publications\", height=400)\n",
    "    num_pages_dist.data = []\n",
    "    num_pages_dist.add_traces(fig3.data)\n",
    "    num_pages_dist.layout = fig3.layout\n",
    "    loading4.value = loading4.value + 1\n",
    "        \n",
    "    update_pies(curr_data2)\n",
    "        \n",
    "\n",
    "# update the pie charts\n",
    "def update_pies(curr_data2):\n",
    "    # update pubtypes\n",
    "    fig = px.pie(values=list(curr_data2[\"PubType\"].value_counts().values), \n",
    "                 names=list(curr_data2[\"PubType\"].value_counts().index), \n",
    "                 title='Publication Types')\n",
    "    pubtype_dist.data = []\n",
    "    pubtype_dist.add_traces(fig.data)\n",
    "    pubtype_dist.layout = fig.layout\n",
    "    loading4.value = loading4.value + 1\n",
    "        \n",
    "    # update doctypes\n",
    "    doctype_counts = get_doctype_counts(curr_data2)\n",
    "    fig1 = px.pie(values=list(doctype_counts.values()), \n",
    "                  names=list(doctype_counts.keys()), \n",
    "                  title='Document Types')\n",
    "    doctype_dist.data = []\n",
    "    doctype_dist.add_traces(fig1.data)\n",
    "    doctype_dist.layout = fig1.layout\n",
    "    loading4.value = loading4.value + 1\n",
    "        \n",
    "    # update cat1\n",
    "    cat1_counts = get_cat_counts(curr_data2, \"Category: Heading 1\")\n",
    "    #cat1_counts = cat1_counts.rename(columns={\"index\": \"Heading\"})\n",
    "    fig2 = px.pie(values=list(cat1_counts.values()), \n",
    "                  names=list(cat1_counts.keys()), \n",
    "                  title='Heading 1 Categories', labels={\"Category: Heading 1\": \"Num Publications\"})\n",
    "    cat1_graph.data = []\n",
    "    cat1_graph.add_traces(fig2.data)\n",
    "    cat1_graph.layout = fig2.layout\n",
    "    loading4.value = loading4.value + 1\n",
    "        \n",
    "    # update cat2\n",
    "    cat2_counts = get_cat_counts(curr_data2, \"Category: Subheadings\")\n",
    "    fig3 = px.pie(values=list(cat2_counts.values()), \n",
    "                  names=list(cat2_counts.keys()), \n",
    "                  title='Subheading Categories')\n",
    "    cat2_graph.data = []\n",
    "    cat2_graph.add_traces(fig3.data)\n",
    "    cat2_graph.layout = fig3.layout\n",
    "    loading4.value = loading4.value + 1\n",
    "        \n",
    "    # update cat3\n",
    "    cat3_counts, all_cat3_counts = get_subject_counts(curr_data2)\n",
    "    fig4 = px.pie(cat3_counts, values=\"Num Publications\", \n",
    "                  names=\"Subject\", \n",
    "                  title='Subject Categories')\n",
    "    cat3_graph.data = []\n",
    "    cat3_graph.add_traces(fig4.data)\n",
    "    cat3_graph.layout = fig4.layout\n",
    "    loading4.value = loading4.value + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display\n",
    "\n",
    "head_dd.observe(head_update, names = 'value')\n",
    "subhead_dd.observe(subhead_update, names = 'value')\n",
    "data_btn2.on_click(select_data_update2)\n",
    "\n",
    "selections2 = widgets.Accordion(children=[data_widgets2, input_widgets2])\n",
    "selections2.set_title(0, 'Data Selection')\n",
    "selections2.set_title(1, 'Display Selection')\n",
    "\n",
    "dist_tab = widgets.Tab([download_dist, num_authors_dist, prop_authors_dist, num_pages_dist])\n",
    "dist_tab.set_title(0, \"Downloads\")\n",
    "dist_tab.set_title(1, 'Num Authors')\n",
    "dist_tab.set_title(2, 'Prop Authors')\n",
    "dist_tab.set_title(3, 'Pages')\n",
    "\n",
    "type_tab = widgets.Tab([pubtype_dist, doctype_dist])\n",
    "type_tab.set_title(0, \"Publication Types\")\n",
    "type_tab.set_title(1, 'Document Types')\n",
    "\n",
    "cat_tab = widgets.Tab([cat1_graph, cat2_graph, cat3_graph])\n",
    "cat_tab.set_title(0, \"Headings\")\n",
    "cat_tab.set_title(1, 'Subheadings')\n",
    "cat_tab.set_title(2, 'Subjects')\n",
    "\n",
    "main_uoft_tab = widgets.Tab([uoft_stats, dist_tab, type_tab, cat_tab])\n",
    "main_uoft_tab.set_title(0, \"Stats\")\n",
    "main_uoft_tab.set_title(1, 'Distributions')\n",
    "main_uoft_tab.set_title(2, 'Journal Types')\n",
    "main_uoft_tab.set_title(3, 'Categories')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46a49e7e1bba48188f451dc3265fc2f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(VBox(children=(SelectionRangeSlider(description='Time Range', index=(0, 59), layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2abc7110a40418aaaefd40d22737625",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, bar_style='info', description='Loading:', max=15)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "911dfec15aca4e1097e7d6cb0eca2aab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(Output(), Tab(children=(FigureWidget({\n",
       "    'data': [], 'layout': {'template': '...'}\n",
       "}), FigureW…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(selections2)\n",
    "display(loading4)\n",
    "display(main_uoft_tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
